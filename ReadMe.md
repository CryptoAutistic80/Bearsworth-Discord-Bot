**Discord Chatbot using OpenAI GPT-3**

This is a chatbot built with Discord API and OpenAI's GPT-3 model. It allows users to chat with the bot using natural language and get responses generated by the model.

**Setup**

Before you can use this chatbot, you need to do the following:

1. Create a Discord bot and get its token. Follow the instructions here to create a new Discord bot and get its token.
2. Set up OpenAI API credentials. Follow the instructions here to sign up for OpenAI and get your API key.
3. Install the required Python packages. Run pip install -r requirements.txt to install the required packages.

**Configuration**

To configure the chatbot, you need to do the following:

Create a file named prompt_parameters.json in the same directory as the script. This file should contain the following parameters -

model: the name of the GPT-3 model to use. You can find the list of available models here.

messages: a list of messages that will be fed to the model as a conversation history. This can be used to give the model some context about the conversation.

max_tokens: the maximum number of tokens that the model can generate for each response.

**Set the following environment variables:**

DISCORD_TOKEN: the token of your Discord bot.
KEY_OPENAI: your OpenAI API key.

**Usage**

To use the chatbot, you need to run the script using the following command:

python main.py

Once the bot is running, you can start a private chat thread with the bot by typing the following command in any Discord channel:

!chat

This will create a new private chat thread with the bot, and you can start chatting with it using natural language.

To end the chat thread, type the following command:

!end

**Limitations**

This chatbot has some limitations, such as:

It only supports one conversation at a time per user.
It only stores the conversation history in memory, so it will be lost if the bot is restarted.
It has a request queue to limit the number of concurrent requests to OpenAI's API and avoid exceeding the rate limit, but it may still hit the rate limit if there are too many requests at once.